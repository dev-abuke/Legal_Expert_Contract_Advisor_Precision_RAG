{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"rag-datasets/mini-bioasq\")\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load generator model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Function to retrieve relevant contexts\n",
    "def retrieve_contexts(question, contexts, k=3):\n",
    "    question_embedding = embedder.encode([question])[0]\n",
    "    context_embeddings = embedder.encode(contexts)\n",
    "    similarities = cosine_similarity([question_embedding], context_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    return [contexts[i] for i in top_k_indices]\n",
    "\n",
    "# Function to generate answer\n",
    "def generate_answer(question, context):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
    "    outputs = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Evaluate RAG system\n",
    "def evaluate_rag(dataset, num_samples=100):\n",
    "    correct = 0\n",
    "    for i in range(num_samples):\n",
    "        sample = dataset['test'][i]\n",
    "        question = sample['question']\n",
    "        contexts = sample['contexts']\n",
    "        true_answer = sample['answers'][0]\n",
    "        \n",
    "        retrieved_contexts = retrieve_contexts(question, contexts)\n",
    "        generated_answer = generate_answer(question, \" \".join(retrieved_contexts))\n",
    "        \n",
    "        if generated_answer.lower() in true_answer.lower():\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / num_samples\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_rag(dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
